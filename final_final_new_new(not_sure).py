# final_stable_recorder.py (åŸºäº weqeqwe.py æˆåŠŸæ¶æ„ï¼ŒåŒ…å«å¯åŠ¨åŒæ­¥çš„æœ€ç»ˆå®Œæ•´ç‰ˆ)

import cv2
import os
import sys
import numpy as np
import torch
import yaml  # è®€å– YAML é…ç½®æª”
from aiortc import RTCSessionDescription, RTCPeerConnection # aiortcæ˜¯ WebRTC çš„ Python å¯¦ä½œ
#RTCPeerConnection ç”¨æ–¼å»ºç«‹ WebRTC é€£ç·š

from ultralytics import YOLO # YOLOv8 ç‰©ä»¶åµæ¸¬æ¨¡å‹
from boxmot.trackers.bytetrack.bytetrack import ByteTrack # ByteTrack è¿½è¹¤å™¨
from collections import defaultdict, deque # ç”¨æ–¼å„²å­˜è¿½è¹¤æ­·å²

import time
import uuid# ç”¨æ–¼ç”¢ç”Ÿå”¯ä¸€çš„è¡Œç¨‹ ID
import requests # ç”¨æ–¼èˆ‡ Flask å¾Œç«¯é€šè¨Š

#asyncio èˆ‡ aiohttp ç”¨æ–¼éåŒæ­¥ WebRTC å½±åƒæ¥æ”¶
#å› ç‚º WebRTC éœ€è¦éåŒæ­¥è™•ç†ä¾†æœ‰æ•ˆç‡åœ°æ¥æ”¶å½±åƒä¸²æµ


#webrtc ä»–çš„è™•ç†æ­¥é©ŸåŒ…å« 1.å»ºç«‹ RTCPeerConnection 2.äº¤æ› SDP 3.æ¥æ”¶å½±åƒ Track
#asynicio ä¸»è¦è™•ç†çš„éƒ¨åˆ†æ˜¯ç­‰å¾…å½±åƒå¹€çš„æ¥æ”¶
#http   å‰‡æ˜¯ç”¨ä¾†èˆ‡ mediamtx ä¼ºæœå™¨äº¤æ› SDP
import asyncio # éåŒæ­¥è™•ç†
import aiohttp # éåŒæ­¥ HTTP è«‹æ±‚
from aiortc import RTCConfiguration, RTCIceServer  #é€™é‚Šå‰‡æ˜¯ç”¨ä¾†è¨­å®š ICE ä¼ºæœå™¨
# ç”šéº¼æ˜¯ICE ä¼ºæœå™¨å‘¢? ICE ä¼ºæœå™¨ç”¨æ–¼å”åŠ© WebRTC é€£ç·šçš„å»ºç«‹
#ç›¸ç•¶æ–¼ä¸­ä»‹ä¼ºæœå™¨ å¹«åŠ©é›™æ–¹æ‰¾åˆ°å½¼æ­¤ é€™è£¡çš„é›™æ–¹ æ˜¯æŒ‡å…©å€‹ WebRTC å®¢æˆ¶ç«¯ ä¸€å€‹pc ä¸€å€‹meadiamtx
#configuraion æ˜¯ç”¨ä¾†è¨­å®š RTCPeerConnection çš„åƒæ•¸
#æˆ‘å€‘åœ¨é€™è£¡è¨­å®š ICE ä¼ºæœå™¨ è®“ WebRTC èƒ½å¤ é †åˆ©ç©¿è¶Š NAT èˆ‡é˜²ç«ç‰†
#é€™æ¨£æ‰èƒ½æˆåŠŸå»ºç«‹ P2P é€£ç·š
#rtcsessiondescription å‰‡æ˜¯ç”¨ä¾†å°è£ SDP è³‡è¨Šçš„ç‰©ä»¶
#SDP æ˜¯ WebRTC ç”¨ä¾†æè¿°å¤šåª’é«”é€£ç·šåƒæ•¸çš„æ ¼å¼
#å®ƒåŒ…å«äº†ç·¨è§£ç¢¼å™¨ ç¶²è·¯ä½å€ç­‰è³‡è¨Š




import traceback # ç”¨æ–¼éŒ¯èª¤è¿½è¹¤
import threading # å¤šåŸ·è¡Œç·’è™•ç†

from queue import Queue, Empty # ç·šç¨‹å®‰å…¨çš„ä½‡åˆ—
#é€™å€‹ç·šç¨‹å°±æ˜¯threading.Thread ä»–èƒ½å¤ è®“æˆ‘å€‘åœ¨èƒŒæ™¯åŸ·è¡Œä»»å‹™
#Queue æ˜¯ç·šç¨‹å®‰å…¨çš„ä½‡åˆ— ç”¨æ–¼åœ¨ä¸åŒç·šç¨‹é–“å‚³éè³‡æ–™

import torch.nn.functional as F #pytorch çš„å‡½å¼åº« ç”¨æ–¼å¼µé‡æ“ä½œ å¼µé‡æ˜¯å¤šç¶­é™£åˆ— é¡ä¼¼ numpy çš„é™£åˆ—
from datetime import datetime # ç”¨æ–¼å–å¾—ç›®å‰æ™‚é–“


import time
# --- 1. è·¯å¾„ä¸é…ç½® ---


try:
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__)) #ç•¶å‰æª”æ¡ˆè·¯å¾‘
    PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, '..')) #å°ˆæ¡ˆæ ¹ç›®éŒ„ #å°±æ˜¯ä¸Šä¸€å±¤ç›®éŒ„stream_yolo
    YNET_PROJECT_PATH = os.path.join(PROJECT_ROOT, 'Ynet_kitti_tracking-master') #Y-Net å°ˆæ¡ˆè·¯å¾‘ #stream_yolo/Ynet_kitti_tracking-master
    if YNET_PROJECT_PATH not in sys.path: #å¦‚æœ YNET_PROJECT_PATH ä¸åœ¨ç³»çµ±è·¯å¾‘ä¸­
        sys.path.append(YNET_PROJECT_PATH) #åŠ å…¥ç³»çµ±è·¯å¾‘ ç³»çµ±è·¯å¾‘
    from model import YNet #Y-Net æ¨¡å‹
    from network import modeling #
    from utils.image_utils import create_arrow_heatmap, get_patch, create_dist_mat, sampling #Y-Net å·¥å…·å‡½å¼
except ImportError as e:
    print(f"é”™è¯¯ï¼šæ— æ³•å¯¼å…¥ Y-Net ç›¸å…³æ¨¡ç»„: {e}")
    print(f"è¯·ç¡®ä¿ 'Ynet_kitti_tracking-master' æ–‡ä»¶å¤¹ä¸æ‚¨çš„ä¸“æ¡ˆæ–‡ä»¶å¤¹ä½äºåŒä¸€å±‚ç›®å½•ä¸‹ã€‚")
    sys.exit(1)

# --- é…ç½®é¸é … --- å¯é¸æ“‡ WebRTC æˆ– æœ¬åœ° Webcam ä½œç‚ºè¼¸å…¥ä¾†æº
INPUT_SOURCE_MODE = "WEBRTC"  # å¯é€‰é¡¹: "WEBRTC" æˆ– "WEBCAM"

# --- é€šç”¨é…ç½® ---
VIDEO_FILES_DIR = "videos"  #å½±ç‰‡å„²å­˜ç›®éŒ„
RECORD_OUTPUT_DIR = os.path.join(CURRENT_DIR, "web_server", VIDEO_FILES_DIR) #ç•¶å‰ä½ç½®çš„ web_server/videos
os.makedirs(RECORD_OUTPUT_DIR, exist_ok=True) #ç¢ºä¿ç›®éŒ„å­˜åœ¨
print(f"å½±ç‰‡å°†å‚¨å­˜è‡³: {RECORD_OUTPUT_DIR}") #åˆ—å°å½±ç‰‡å„²å­˜ç›®éŒ„

FLASK_BACKEND_URL = "http://192.168.196.73:5000"  #Flask å¾Œç«¯ä¼ºæœå™¨ URL é€™zerotierçµ¦çš„ æ‰€ä»¥åªè¦åœ¨åŒä¸€å€‹ç¶²è·¯å°±å¯ä»¥é€£åˆ° androidåªè¦zerotierå°±å¯ä»¥é€£åˆ°é€™å€‹ip æŠŠgpså‚³å›å»
#é€™å€‹ ip æ˜¯å¾Œç«¯ä¼ºæœå™¨çš„ä½å€

RECORDING_STATUS_ENDPOINT = f"{FLASK_BACKEND_URL}/recording_status"  #POST /recording_status
UPLOAD_VIDEO_ENDPOINT = f"{FLASK_BACKEND_URL}/upload_recorded_video" #POST /upload_recorded_video
NOTIFY_DANGER_ENDPOINT = f"{FLASK_BACKEND_URL}/notify_danger" #POST /notify_danger

YOLO_MODEL_PATH = os.path.join(YNET_PROJECT_PATH, 'yolov8m.pt') #YOLOv8 æ¨¡å‹è·¯å¾‘
YNET_MODEL_PATH = os.path.join(YNET_PROJECT_PATH, 'pretrained_models/kitti_ynet_baseline_s13_best.pt') #Y-Net æ¨¡å‹è·¯å¾‘
SEG_MODEL_PATH = os.path.join(YNET_PROJECT_PATH, 'segmentation_models/best_deeplabv3plus_mobilenet_cityscapes_os16.pth')#èªç¾©åˆ†å‰²æ¨¡å‹è·¯å¾‘
YNET_CONFIG_PATH = os.path.join(YNET_PROJECT_PATH, r'kitti_train_data/config/kitti.yaml') #Y-Net é…ç½®æª”è·¯å¾‘

SEGMENTATION_INTERVAL = 1 #æ¯ N å¹€åŸ·è¡Œä¸€æ¬¡èªç¾©åˆ†å‰²
MODEL_INPUT_WIDTH = 640 #YOLO èˆ‡ Y-Net æ¨¡å‹è¼¸å…¥å°ºå¯¸
MODEL_INPUT_HEIGHT = 192 #YOLO èˆ‡ Y-Net æ¨¡å‹è¼¸å…¥å°ºå¯¸
CLASSES_TO_TRACK = [0, 1, 2, 3, 5, 7] # å¢åŠ äº† 0: person
# SAVE_VIDEO = True #æ˜¯å¦å„²å­˜éŒ„å½±å½±ç‰‡  <-- é€™å€‹è®Šæ•¸åœ¨æ–°é‚è¼¯ä¸­ä¸å†éœ€è¦

# --- WebRTC é…ç½® ---
mediamtx_base_url = "http://192.168.196.73:8889" # è¿œç«¯æ ‘è“æ´¾çš„# 192.168.196.73  ZeroTier IP   #WIFI 10.21.78.41:8889
stream_paths = ["cam0", "cam1"]

# --- æœ¬åœ° Webcam é…ç½® ---
local_camera_indices = [0] #æœ¬åœ°æ”åƒé ­

# ---å¯«å…¥å½±ç‰‡çš„åŸ·è¡Œç·’--- (ä¿æŒæ‚¨åŸæœ‰çš„ç‰ˆæœ¬ï¼Œå®Œå…¨ä¸è®Š)
class VideoWriterThread(threading.Thread): #å½±ç‰‡å¯«å…¥åŸ·è¡Œç·’
    def __init__(self, output_path, frame_size, fps=10.0):
        super().__init__() #å‘¼å«çˆ¶é¡åˆ¥threading.Threadçš„åˆå§‹åŒ–æ–¹æ³•
        self.daemon = True #è¨­ç½®ç‚ºå®ˆè­·ç·šç¨‹ ä½œç”¨æ˜¯ç•¶ä¸»ç·šç¨‹çµæŸæ™‚ è‡ªå‹•çµæŸé€™å€‹ç·šç¨‹ é€™æ¨£å°±ä¸ç”¨å¯« .join() ä¾†ç­‰å¾…ç·šç¨‹çµæŸ
        self.output_path, self.frame_size, self.fps = output_path, frame_size, fps #å½±ç‰‡è¼¸å‡ºè·¯å¾‘ å¹€å°ºå¯¸ å¹€ç‡
        self.write_queue = Queue(maxsize=120) #å¯«å…¥ä½‡åˆ— æœ€å¤š120å¹€
        self.running = True #åŸ·è¡Œç‹€æ…‹
        self.writer = None #å½±ç‰‡å¯«å…¥å™¨
    def run(self): #ç‚ºä»€éº¼ä¸€å®šè¦è¦†å¯« run æ–¹æ³•? å› ç‚º threading.Thread çš„ run æ–¹æ³•æ˜¯ç©ºçš„
        #threading.Thread åœ¨æ–°åŸ·è¡Œç·’å…§åŸ·è¡Œçš„ã€Œå…¥å£å‡½å¼ã€ã€‚è¦†å¯« run() çš„ç›®çš„å°±æ˜¯æŠŠä½ è¦åœ¨é‚£å€‹æ–°åŸ·è¡Œç·’è£¡åšçš„å·¥ä½œå¯«é€²å»
        try:
            fourcc = cv2.VideoWriter_fourcc(*'avc1') #ä½¿ç”¨ AVC1 ç·¨ç¢¼å™¨
            self.writer = cv2.VideoWriter(self.output_path, fourcc, self.fps, self.frame_size)#å»ºç«‹å½±ç‰‡å¯«å…¥å™¨
            if not self.writer.isOpened(): raise IOError("AVC1 failed") #æª¢æŸ¥æ˜¯å¦æˆåŠŸé–‹å•Ÿ
            print(f"å½±ç‰‡å¯«å…¥åŸ·è¡Œç·’(avc1)å·²å•Ÿå‹•: {os.path.basename(self.output_path)}")
        except Exception:
            print(f"è­¦å‘Š: AVC1 ç·¨ç¢¼å™¨ä¸å¯ç”¨, é™ç´šè‡³ MP4V for {os.path.basename(self.output_path)}")
            fourcc = cv2.VideoWriter_fourcc(*'mp4v')
            self.writer = cv2.VideoWriter(self.output_path, fourcc, self.fps, self.frame_size)#å»ºç«‹å½±ç‰‡å¯«å…¥å™¨
        while self.running or not self.write_queue.empty(): #æŒçºŒå¯«å…¥ç›´åˆ°åœæ­¢ä¸”ä½‡åˆ—æ¸…ç©º ä½‡åˆ—æ˜¯
            try:
                frame = self.write_queue.get(timeout=1) #ç­‰å¾…1ç§’å–å‡ºä¸€å¹€
                if self.writer: self.writer.write(frame) #å¯«å…¥å½±ç‰‡
            except Empty: continue
        if self.writer: self.writer.release() #é‡‹æ”¾å½±ç‰‡å¯«å…¥å™¨
        print(f"å½±ç‰‡ {os.path.basename(self.output_path)} å¯«å…¥å®Œæˆã€‚")
    def add_frame_to_queue(self, frame): #åŠ å…¥å¹€åˆ°ä½‡åˆ—
        if not self.write_queue.full(): self.write_queue.put_nowait(frame) #éé˜»å¡åŠ å…¥
    def stop(self): self.running = False#åœæ­¢å¯«å…¥

# --- notify_backend å‡½å¼ (ä¿æŒæ‚¨åŸæœ‰çš„ç‰ˆæœ¬ï¼Œå®Œå…¨ä¸è®Š) ---
def notify_backend(endpoint, data): #é€šçŸ¥ Flask å¾Œç«¯
    try:
        response = requests.post(endpoint, json=data, timeout=5) #POST è«‹æ±‚
        response.raise_for_status() #æª¢æŸ¥æ˜¯å¦æˆåŠŸ
        print(f"æˆåŠŸé€šçŸ¥å¾Œç«¯ {os.path.basename(endpoint)}ï¼Œç‹€æ…‹ç¢¼: {response.status_code}") #åˆ—å°æˆåŠŸè¨Šæ¯
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"éŒ¯èª¤ï¼šé€šçŸ¥å¾Œç«¯ {os.path.basename(endpoint)} å¤±æ•—: {e}")
        return None

# --- webrtc_receiver_task å‡½å¼ (ä¿æŒæ‚¨åŸæœ‰çš„ç‰ˆæœ¬ï¼Œå®Œå…¨ä¸è®Š) ---
async def webrtc_receiver_task(path, frame_queue, shutdown_event):
    # ... (æ‚¨æ‰€æœ‰çš„ webrtc_receiver_task ç¨‹å¼ç¢¼éƒ½ä¿æŒä¸è®Š) ...
    pc = RTCPeerConnection()
    @pc.on("track")
    async def on_track(track):
        if track.kind == "video":
            while not shutdown_event.is_set():
                try:
                    frame = await asyncio.wait_for(track.recv(), timeout=10)
                    if not frame_queue.full():
                        frame_queue.put_nowait(frame.to_ndarray(format="bgr24"))
                except asyncio.TimeoutError:
                    print(f"[{path}] æ¥æ”¶å½±åƒå¹€è¶…æ™‚ã€‚")
                    break
                except Exception: break
    try:
        url = f"{mediamtx_base_url}/{path}/whep"
        print(f"[{path}] æ­£åœ¨é€£æ¥åˆ° WebRTC: {url} ...")
        pc.addTransceiver("video", direction="recvonly")
        offer = await pc.createOffer()
        await pc.setLocalDescription(offer)
        async with aiohttp.ClientSession() as session:
            async with session.post(url, data=pc.localDescription.sdp, headers={"Content-Type": "application/sdp"}) as resp:
                if resp.status != 201:
                    print(f"[{path}] é€£ç·šå¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {resp.status}")
                    return
                answer_sdp = await resp.text()
                await pc.setRemoteDescription(RTCSessionDescription(sdp=answer_sdp, type="answer"))
                print(f"[{path}] WebRTC é€£ç·šæˆåŠŸï¼")
        while not shutdown_event.is_set():
            await asyncio.sleep(0.5)
    except asyncio.CancelledError: pass
    finally:
        await pc.close()
        print(f"[{path}] WebRTC é€£ç·šå·²é—œé–‰ã€‚")

# --- webrtc_receiver_thread & webcam_receiver_thread å‡½å¼ (ä¿æŒæ‚¨åŸæœ‰çš„ç‰ˆæœ¬ï¼Œå®Œå…¨ä¸è®Š) ---
def webrtc_receiver_thread(path, frame_queue, shutdown_event):
    # ... (æ‚¨æ‰€æœ‰çš„ webrtc_receiver_thread ç¨‹å¼ç¢¼éƒ½ä¿æŒä¸è®Š) ...
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    # ... (å…¶é¤˜ receiver_task çš„å…§å®¹ä¹Ÿå®Œå…¨ä¸è®Š) ...
    async def receiver_task():
        while not shutdown_event.is_set():
            pc = None
            try:
                ice_servers = [RTCIceServer(urls=["stun:stun.l.google.com:19302"])]
                config = RTCConfiguration(iceServers=ice_servers)
                pc = RTCPeerConnection(configuration=config)
                pc.RTCP_REPORTS_DEFAULT = True
                first_frame_received = asyncio.Queue(maxsize=1)
                @pc.on("track")
                async def on_track(track):
                    if track.kind == "video":
                        try:
                            first_frame = await asyncio.wait_for(track.recv(), timeout=15.0)
                            if not frame_queue.full(): frame_queue.put_nowait(first_frame.to_ndarray(format="bgr24"))
                            await first_frame_received.put(True)
                            while not shutdown_event.is_set():
                                frame = await asyncio.wait_for(track.recv(), timeout=5.0)
                                if not frame_queue.full(): frame_queue.put_nowait(frame.to_ndarray(format="bgr24"))
                        except (asyncio.TimeoutError, Exception):
                            if first_frame_received.empty(): await first_frame_received.put(False)
                url = f"{mediamtx_base_url}/{path}/whep"
                pc.addTransceiver("video", direction="recvonly")
                offer = await pc.createOffer()
                await pc.setLocalDescription(offer)
                async with aiohttp.ClientSession() as session:
                    async with session.post(url, data=pc.localDescription.sdp, headers={"Content-Type": "application/sdp"}) as resp:
                        if resp.status != 201:
                            await asyncio.sleep(5); continue
                        answer_sdp = await resp.text()
                        await pc.setRemoteDescription(RTCSessionDescription(sdp=answer_sdp, type="answer"))
                success = await asyncio.wait_for(first_frame_received.get(), timeout=15.0)
                if not success: continue
                while not shutdown_event.is_set() and pc.connectionState in ["connected", "connecting"]:
                    await asyncio.sleep(1)
            except (asyncio.CancelledError, Exception): pass
            finally:
                if pc: await pc.close()
                if not shutdown_event.is_set(): await asyncio.sleep(5)
    loop.run_until_complete(receiver_task())


def webcam_receiver_thread(camera_index, frame_queue, shutdown_event):
    # ... (æ‚¨æ‰€æœ‰çš„ webcam_receiver_thread ç¨‹å¼ç¢¼éƒ½ä¿æŒä¸è®Š) ...
    print(f"ğŸ¥ æ­£åœ¨å•Ÿå‹•æœ¬åœ°æ”åƒé ­ #{camera_index} ...")
    cap = cv2.VideoCapture(camera_index)
    if not cap.isOpened():
        print(f"éŒ¯èª¤ï¼šç„¡æ³•é–‹å•Ÿæ”åƒé ­ #{camera_index}")
        return
    while not shutdown_event.is_set():
        ret, frame = cap.read()
        if not ret: break
        if not frame_queue.full(): frame_queue.put_nowait(frame)
        time.sleep(0.01)
    cap.release()
    print(f"ğŸ“· æ”åƒé ­åŸ·è¡Œç·’ #{camera_index} å·²çµæŸã€‚")

# --- 3. ä¸»ç¨‹å¼ ---
def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨çš„è¨ˆç®—è£ç½®: {device}")
    trip_id = f"trip_{uuid.uuid4().hex[:8]}"
    print(f"====== æœ¬æ¬¡è¡Œç¨‹ ID: {trip_id} ======")
    notify_backend(RECORDING_STATUS_ENDPOINT, {"session_id": trip_id, "status": "start"})

    shutdown_event = threading.Event()
    frame_queues, receiver_threads = {}, []

    active_paths = [] # ç‚ºäº†è®“ IDE ä¸å ±éŒ¯ï¼Œå…ˆåˆå§‹åŒ–
    if INPUT_SOURCE_MODE == "WEBRTC":
        active_paths = stream_paths
        print(f"--- å•Ÿå‹• WebRTC æ¨¡å¼ï¼Œè™•ç†ä¸²æµ: {active_paths} ---")
        for path in active_paths:
            frame_queues[path] = Queue(maxsize=30)
            thread = threading.Thread(target=webrtc_receiver_thread, args=(path, frame_queues[path], shutdown_event), daemon=True)
            receiver_threads.append(thread); thread.start()

    elif INPUT_SOURCE_MODE == "WEBCAM":
        active_paths = [f"webcam{i}" for i in local_camera_indices]
        print(f"--- å•Ÿå‹•æœ¬åœ° Webcam æ¨¡å¼ï¼Œè™•ç†é¡é ­: {local_camera_indices} ---")
        for i, path in zip(local_camera_indices, active_paths):
            frame_queues[path] = Queue(maxsize=30)  # ä¿æŒèˆ‡ WebRTC æ¨¡å¼ä¸€è‡´çš„ä½‡åˆ—å¤§å°
            thread = threading.Thread(target=webcam_receiver_thread, args=(i, frame_queues[path], shutdown_event),
                                      daemon=True)
            receiver_threads.append(thread)
            thread.start()
    else:
        print(f"éŒ¯èª¤ï¼šæœªçŸ¥çš„ INPUT_SOURCE_MODE: {INPUT_SOURCE_MODE}"); return

    # --- æ¨¡å‹è¼‰å…¥éƒ¨åˆ† (ä¿æŒæ‚¨åŸæœ‰çš„ç‰ˆæœ¬ï¼Œå®Œå…¨ä¸è®Š) ---
    print("--- æ­£åœ¨è¼‰å…¥æ‰€æœ‰ AI æ¨¡å‹... ---")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    yolo_model = YOLO(YOLO_MODEL_PATH)
    with open(YNET_CONFIG_PATH, 'r', encoding='utf-8') as file:
        params = yaml.load(file, Loader=yaml.FullLoader)
    obs_len = params['obs_len']
    ynet_model = YNet(obs_len=obs_len, pred_len=params['pred_len'], params=params)
    ynet_model.load(YNET_MODEL_PATH)
    ynet_model.model.to(device).eval()
    checkpoint = torch.load(SEG_MODEL_PATH, map_location=device)
    seg_model = modeling.deeplabv3plus_mobilenet(num_classes=19, output_stride=16)
    seg_model.load_state_dict({k.replace('module.',''):v for k,v in checkpoint['model_state'].items()})
    seg_model.to(device).eval()
    tracker = ByteTrack(track_thresh=0.1,match_thresh=0.4) # åˆå§‹åŒ– ByteTrack è¿½è¹¤å™¨ #0.1æ˜¯åµæ¸¬é–€æª»å€¼ è¶Šä½è¶Šéˆæ• ä½†ä¹Ÿå¯èƒ½èª¤è¿½è¹¤ 0.6æ˜¯åŒ¹é…é–€æª»å€¼ è¶Šé«˜è¶Šåš´æ ¼
    input_template = torch.Tensor(create_dist_mat(size=2000)).to(device)
    print("--- æ‰€æœ‰æ¨¡å‹è¼‰å…¥å®Œæˆï¼ ---")

    # --- åˆå§‹åŒ– AI åˆ†æèˆ‡æ–°ç‰ˆéŒ„å½±ç›¸é—œçš„è®Šæ•¸ ---
    all_track_histories = {path: defaultdict(lambda: deque(maxlen=obs_len)) for path in active_paths}
    all_track_predictions = {path: {} for path in active_paths}
    all_frame_idx = {path: 0 for path in active_paths}
    all_cached_seg_maps = {path: None for path in active_paths}
    last_danger_notify_time = {path: 0 for path in active_paths}

    # ã€ã€ã€ æ ¸å¿ƒä¿®æ”¹ 1ï¼šåˆå§‹åŒ–äº‹ä»¶éŒ„å½±è®Šæ•¸ ã€‘ã€‘ã€‘
    PRE_EVENT_SECONDS = 5
    POST_EVENT_SECONDS = 60
    EVENT_COOLDOWN_SECONDS = 30  # <--- ã€ã€ã€ æ–°å¢ ã€‘ã€‘
    frame_buffers = {path: deque(maxlen=int(PRE_EVENT_SECONDS * 15)) for path in active_paths} # å‡è¨­  FPS
    event_recording_status = {
        path: {"writer": None, "stop_time": 0} for path in active_paths
    }

    # --- ç­‰å¾…ç¬¬ä¸€å¹€ (ä¿æŒæ‚¨åŸæœ‰çš„ç‰ˆæœ¬ï¼Œå®Œå…¨ä¸è®Š) ---
    print("\n--- ç­‰å¾…æ‰€æœ‰å½±åƒä¾†æºçš„ç¬¬ä¸€å¹€ï¼Œæœ€å¤šç­‰å¾… 30 ç§’... ---")
    initial_frames = {}
    time.sleep(2)
    for path in active_paths:
        try:
            print(f"æ­£åœ¨ç­‰å¾… [{path}] çš„ç¬¬ä¸€å¹€...")
            frame = frame_queues[path].get(timeout=30)
            initial_frames[path] = frame
            print(f"âœ… æˆåŠŸæ¥æ”¶åˆ° [{path}] çš„ç¬¬ä¸€å¹€ï¼")
        except Empty:
            print(f"âŒ è­¦å‘Šï¼šç­‰å¾… [{path}] çš„ç¬¬ä¸€å¹€è¶…æ™‚ï¼Œå°‡å¿½ç•¥æ­¤ä¸²æµã€‚")

    active_paths = list(initial_frames.keys())
    if not active_paths:
        print("éŒ¯èª¤ï¼šæ²’æœ‰ä»»ä½•å½±åƒä¾†æºæˆåŠŸé€£æ¥ã€‚ç¨‹å¼å³å°‡é€€å‡ºã€‚")
        shutdown_event.set()
        return

    # --- ä¸»è¿´åœˆ ---
    try:
        while not shutdown_event.is_set():
            for path in active_paths:
                if path in initial_frames:
                    frame_orig = initial_frames.pop(path)
                else:
                    try:
                        frame_orig = None
                        # å¦‚æœä½‡åˆ—ä¸­çš„å¹€è¶…éä¸€å®šæ•¸é‡ (ä¾‹å¦‚ 5)ï¼Œå°±æ¸…ç©ºèˆŠçš„ï¼Œåªæ‹¿æœ€æ–°çš„
                        if frame_queues[path].qsize() > 5:
                            # print(f"[{path}] è™•ç†å»¶é²ï¼Œæ­£åœ¨ä¸Ÿæ£„èˆŠå¹€...")
                            while not frame_queues[path].empty():
                                try:
                                    frame_orig = frame_queues[path].get_nowait()
                                except Empty:
                                    break
                        else:
                            frame_orig = frame_queues[path].get_nowait()

                        if frame_orig is None:
                            continue
                    except Empty:
                        continue

                # --- å¾é€™è£¡é–‹å§‹ï¼ŒAI åˆ†æå’Œç¹ªåœ–çš„ç¨‹å¼ç¢¼éƒ½ä¿æŒä¸è®Š ---
                window_name = f"Intelligent Recorder - {path}"
                track_histories = all_track_histories[path]
                track_predictions = all_track_predictions[path]
                frame_idx = all_frame_idx[path]
                cached_seg_map = all_cached_seg_maps[path]

                canvas = frame_orig.copy()
                all_frame_idx[path] += 1
                frame_model_size = cv2.resize(frame_orig, (MODEL_INPUT_WIDTH, MODEL_INPUT_HEIGHT))

                # ... (æ‰€æœ‰ YOLO, ByteTrack, åˆ†å‰², è»Œè·¡é æ¸¬çš„ç¨‹å¼ç¢¼éƒ½ä¿æŒä¸è®Š) ...
                results = yolo_model(frame_model_size, conf=0.1,verbose=False, classes=CLASSES_TO_TRACK)
                detections = results[0].boxes.data.cpu().numpy()
                tracked_objects = tracker.update(detections, frame_model_size)
                active_track_ids = {int(obj[4]) for obj in tracked_objects}
                track_predictions = {tid: pred for tid, pred in track_predictions.items() if tid in active_track_ids}
                if frame_idx % SEGMENTATION_INTERVAL == 0 or cached_seg_map is None:
                    img_rgb = cv2.cvtColor(frame_model_size, cv2.COLOR_BGR2RGB)
                    img_tensor = torch.from_numpy(img_rgb.astype(np.float32)/255.0).permute(2,0,1)
                    mean, std = torch.tensor([0.485,0.456,0.406],device=device).view(3,1,1), torch.tensor([0.229,0.224,0.225],device=device).view(3,1,1)
                    seg_input_tensor = ((img_tensor.to(device) - mean)/std).unsqueeze(0)
                    with torch.no_grad():
                        seg_logits = seg_model(seg_input_tensor)
                        if isinstance(seg_logits, dict): seg_logits = seg_logits['out']
                        cached_seg_map = torch.argmax(seg_logits.squeeze(), dim=0).cpu().numpy()
                    all_cached_seg_maps[path] = cached_seg_map
                tracks_to_predict = []
                for obj in tracked_objects: ###
                    x1, y1, x2, y2, track_id = obj[:5]
                    track_histories[int(track_id)].append([(x1+x2)/2, y2])
                    if len(track_histories[int(track_id)]) == obs_len:
                        tracks_to_predict.append(int(track_id))
                if tracks_to_predict:
                    with torch.no_grad():
                        num_to_predict = len(tracks_to_predict)
                        batch_hist = torch.from_numpy(
                            np.array([list(track_histories[tid]) for tid in tracks_to_predict])).float().to(device)
                        vel = batch_hist[:, 1:] - batch_hist[:, :-1]
                        obs_vel = torch.cat([torch.zeros((num_to_predict, 1, 2), device=device), vel], dim=1)
                        acc = obs_vel[:, 1:] - obs_vel[:, :-1]
                        obs_acc = torch.cat([torch.zeros((num_to_predict, 1, 2), device=device), acc], dim=1)
                        h_ynet, w_ynet = MODEL_INPUT_HEIGHT, MODEL_INPUT_WIDTH
                        seg_map_onehot = F.one_hot(torch.from_numpy(cached_seg_map).long().to(device), 19)
                        seg_map_batch = seg_map_onehot.permute(2, 0, 1).float().unsqueeze(0).repeat(num_to_predict, 1,
                                                                                                    1, 1)
                        vel_map = torch.stack([torch.stack(
                            [create_arrow_heatmap(h_ynet, w_ynet, c[0], c[1], v[0], v[1], device=device) for c, v in
                             zip(batch_hist[:, i, :], obs_vel[:, i, :])]) for i in range(obs_len)], dim=1)
                        acc_map = torch.stack([torch.stack(
                            [create_arrow_heatmap(h_ynet, w_ynet, c[0], c[1], a[0], a[1], device=device) for c, a in
                             zip(batch_hist[:, i, :], obs_acc[:, i, :])]) for i in range(obs_len)], dim=1)
                        features = ynet_model.model.pred_features(torch.cat([seg_map_batch, vel_map, acc_map], dim=1))
                        pred_waypoint = ynet_model.model.pred_goal(features)[:, params['waypoints']]
                        pred_waypoint_sm = ynet_model.model.sigmoid(pred_waypoint / params['temperature'])
                        goal_samples = sampling(pred_waypoint_sm[:, -1:],
                                                num_samples=params.get('num_goals', 20)).permute(2, 0, 1, 3)
                        goal_scores = torch.stack([torch.stack([pred_waypoint_sm[
                                                                    i, -1, torch.clamp(g[i, 0, 1].long(), 0,
                                                                                       h_ynet - 1), torch.clamp(
                                                                        g[i, 0, 0].long(), 0, w_ynet - 1)] for i in
                                                                range(num_to_predict)]) for g in goal_samples])
                        future_samples = []
                        for waypoint in goal_samples:
                            waypoint_map = get_patch(input_template, waypoint.reshape(-1, 2).cpu().numpy(), h_ynet,
                                                     w_ynet).reshape([-1, 1, h_ynet, w_ynet])
                            traj_input = [torch.cat([feat,
                                                     F.interpolate(waypoint_map, size=feat.shape[2:], mode='bilinear',
                                                                   align_corners=False)], dim=1) for feat in features]
                            future_samples.append(ynet_model.model.softargmax(ynet_model.model.pred_traj(traj_input)))
                        future_samples = torch.stack(future_samples)
                        best_indices = torch.argmax(goal_scores, dim=0)

                        # --- é€™è£¡å°±æ˜¯è¢«åˆªæ‰çš„é—œéµå®šç¾© ---
                        best_future = future_samples.permute(1, 0, 2, 3)[torch.arange(num_to_predict), best_indices]

                        for i, track_id in enumerate(tracks_to_predict):
                            track_predictions[track_id] = best_future[i].cpu().numpy()

                orig_h, orig_w = frame_orig.shape[:2]
                w_scale, h_scale = orig_w/MODEL_INPUT_WIDTH, orig_h/MODEL_INPUT_HEIGHT

                # ã€ã€ã€ æ ¸å¿ƒä¿®æ”¹ 2ï¼šç”¨æ–°çš„äº‹ä»¶éŒ„å½±é‚è¼¯æ›¿æ›èˆŠçš„éŒ„å½±å’Œé€šçŸ¥é‚è¼¯ ã€‘ã€‘ã€‘
                # 1. å°‡æ¯ä¸€å¹€éƒ½å…ˆå­˜å…¥ç¼“å†²åŒº
                frame_buffers[path].append(canvas)

                # 2. å‹•æ…‹å®šç¾©å±éšªå€åŸŸä¸¦åˆ¤æ–·
                height, width, _ = canvas.shape
                danger_zone_poly = np.array([
                    [int(width * 0.25), int(height * 0.6)], [int(width * 0.75), int(height * 0.6)],
                    [int(width * 0.95), height - 1], [int(width * 0.05), height - 1]
                ], np.int32)
                is_danger = any(cv2.pointPolygonTest(danger_zone_poly, (int(p[0]), int(p[1])), False) >= 0 for tid in
                                track_predictions for p in (track_predictions[tid] * [w_scale, h_scale]))

                # 3. æ ¸å¿ƒéŒ„å½±èˆ‡é€šçŸ¥é‚è¼¯
                current_time = time.time()
                status = event_recording_status[path]

                if is_danger:
                    if status["writer"] is None:
                        print(f"[{path}] ğŸš¨ å±éšªäº‹ä»¶è§¸ç™¼ï¼é–‹å§‹éŒ„è£½...")
                        save_path = os.path.join(RECORD_OUTPUT_DIR, f"EVENT_{path}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4")
                        status["writer"] = VideoWriterThread(save_path, (width, height))
                        status["writer"].start()
                        for frame_in_buffer in list(frame_buffers[path]):
                            status["writer"].add_frame_to_queue(frame_in_buffer)

                        if current_time - last_danger_notify_time.get(path, 0) > 10:
                            last_danger_notify_time[path] = current_time
                            print(f"[{path}] ç™¼é€å±éšªé€šçŸ¥ä¸¦å†·å» 10 ç§’ã€‚")
                            event_timestamp_str = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'
                            danger_data = {"trip_id": trip_id, "event_type": "è»Œè·¡é æ¸¬è­¦å‘Š", "description": f"é¡é ­ [{path}] åµæ¸¬åˆ°æœ‰ç‰©é«”è»Œè·¡é€²å…¥å±éšªå€åŸŸï¼","timestamp": event_timestamp_str}
                            threading.Thread(target=notify_backend, args=(NOTIFY_DANGER_ENDPOINT, danger_data)).start()

                    status["stop_time"] = current_time + POST_EVENT_SECONDS


                elif status["writer"] is not None and current_time > status["stop_time"]:
                    print(f"[{path}] âœ… äº‹ä»¶çµæŸï¼Œåœæ­¢éŒ„å½±ã€‚é€²å…¥ {EVENT_COOLDOWN_SECONDS} ç§’å†·å»æœŸã€‚")
                    status["last_event_end_time"] = current_time  # <--- ã€ã€ã€ æ–°å¢ ã€‘ã€‘ã€‘
                    writer_to_stop = status["writer"]
                    writer_to_stop.stop()


                    # åœ¨èƒŒæ™¯åŸ·è¡Œç·’ä¸­è™•ç†å½±ç‰‡ä¸Šå‚³
                    def final_upload(writer, trip_id, path):
                        writer.join() # ç­‰å¾…å½±ç‰‡å¯«å…¥å®Œæˆ
                        video_filename = os.path.basename(writer.output_path)
                        relative_path = os.path.join(VIDEO_FILES_DIR, video_filename).replace("\\","/")
                        upload_data = { "trip_id": trip_id, "path": path, "relative_path": relative_path, "title": f"å±éšªäº‹ä»¶ - {path}", "date": datetime.now().strftime('%m/%d'), "description": f"åµæ¸¬åˆ°å±éšªäº‹ä»¶çš„ç‰‡æ®µéŒ„å½±ã€‚"}
                        notify_backend(UPLOAD_VIDEO_ENDPOINT, upload_data)

                    threading.Thread(target=final_upload, args=(writer_to_stop, trip_id, path)).start()
                    status["writer"] = None

                if status["writer"] is not None:
                    status["writer"].add_frame_to_queue(canvas)

                # 4. ç¹ªè£½è¦–è¦ºæ•ˆæœ (é€™éƒ¨åˆ†é‚è¼¯ä¿æŒä¸è®Šï¼Œåªæ˜¯ç¾åœ¨å®ƒåœ¨æ–°çš„éŒ„å½±é‚è¼¯å¡Šä¹‹å¾Œ)
                zone_color = (0, 0, 255) if is_danger else (0, 255, 0)
                cv2.polylines(canvas, [danger_zone_poly], True, zone_color, 3)
                # if is_danger:
                    # text = "!!! WARNING !!!"
                    # text_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_TRIPLEX, 2, 3)
                    # cv2.putText(canvas, text, ((width - text_size[0]) // 2, int(height * 0.2)), cv2.FONT_HERSHEY_TRIPLEX, 2, (0, 0, 255), 3)

                # --- ç¹ªè£½è»Œè·¡å’Œæ¡†ç·šçš„ç¨‹å¼ç¢¼ (ä¿æŒæ‚¨åŸæœ‰çš„ç‰ˆæœ¬ï¼Œå®Œå…¨ä¸è®Š) ---
                for obj in tracked_objects:
                    track_id = int(obj[4])
                    x1_m, y1_m, x2_m, y2_m = [int(p) for p in obj[:4]]

                    # åå‘ç¸®æ”¾è¿½è¹¤æ¡†
                    x1_o, y1_o = int(x1_m * w_scale), int(y1_m * h_scale)
                    x2_o, y2_o = int(x2_m * w_scale), int(y2_m * h_scale)
                    cv2.rectangle(canvas, (x1_o, y1_o), (x2_o, y2_o), (0, 255, 0), 2)
                    cv2.putText(canvas, f"ID:{track_id}", (x1_o, y1_o - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0),
                                2)

                    # b. è·å–å½“å‰ç‰©ä»¶çš„æ‰€æœ‰ç›¸å…³èµ„æ–™
                    history_deq = track_histories.get(track_id)
                    prediction_arr = track_predictions.get(track_id)

                    # c. å…ˆç»˜åˆ¶å†å²è½¨è¿¹ (è“è‰²)
                    obs_orig = None
                    if history_deq and len(history_deq) > 0:
                        history_np = np.array(history_deq)
                        # ã€æ³¨æ„ã€‘history_deq å·²ç»æ˜¯æ¨¡å‹åæ ‡ç³»ï¼Œéœ€è¦åå‘ç¼©æ”¾
                        obs_orig = (history_np * [w_scale, h_scale]).astype(np.int32)
                        for k in range(len(obs_orig) - 1):
                            cv2.line(canvas, tuple(obs_orig[k]), tuple(obs_orig[k + 1]), (255, 100, 0), 2)

                    # d. ã€åŒé‡éªŒè¯ä¸åŠ¨æ€æ­¥é•¿é€»è¾‘ã€‘
                    #    åªæœ‰å½“ã€åŒæ—¶ã€‘æœ‰å†å²å’Œé¢„æµ‹æ—¶ï¼Œæ‰è¿›è¡Œåˆ¤æ–­
                    if prediction_arr is not None and history_deq and len(history_deq) >= 2:

                        # --- I. è¡Œä¸ºéªŒè¯ï¼šæ£€æŸ¥æ˜¯å¦â€œè¿´è½¬â€ (åœ¨æ¨¡å‹åæ ‡ç³»ä¸‹è¿›è¡Œ) ---
                        current_pos_model = history_deq[-1]
                        prev_pos_model = history_deq[-2]
                        current_velocity_x = current_pos_model[0] - prev_pos_model[0]

                        predicted_end_point_model = prediction_arr[-1]
                        predicted_direction_x = predicted_end_point_model[0] - current_pos_model[0]

                        # is_reversing çš„åˆ¤æ–­é€»è¾‘ä¿æŒä¸å˜
                        is_reversing = current_velocity_x * predicted_direction_x < -1.0

                        # å¦‚æœæ˜¯è¿´è½¬ï¼Œå°±ç›´æ¥è·³è¿‡è¿™ä¸ªç‰©ä½“çš„é¢„æµ‹ç»˜åˆ¶
                        if is_reversing:
                            continue

                        # --- II. åŠ¨æ€æ­¥é•¿ï¼šå¦‚æœä¸æ˜¯è¿´è½¬ï¼Œå†è®¡ç®—åº”è¯¥ç”»å¤šé•¿ (åœ¨æ¨¡å‹åæ ‡ç³»ä¸‹è¿›è¡Œ) ---
                        current_x_model = current_pos_model[0]

                        distance_to_edge = min(current_x_model, MODEL_INPUT_WIDTH - current_x_model)

                        min_pred_steps = 3
                        # ä¿¡å¿ƒæ¯”ä¾‹ï¼šåœ¨ä¸­å¿ƒä¸º 1.0ï¼Œåœ¨è¾¹ç¼˜ä¸º 0.0
                        confidence_ratio = np.clip(distance_to_edge / (MODEL_INPUT_WIDTH / 2.0), 0.0, 1.0)
                        dynamic_pred_len = int(min_pred_steps + (8 - min_pred_steps) * confidence_ratio)

                        # --- III. æˆªæ–­å¹¶ç»˜åˆ¶ ---
                        pred_model_truncated = prediction_arr[:dynamic_pred_len]

                        # å°†æˆªæ–­åçš„è½¨è¿¹åå‘ç¼©æ”¾å›åŸå§‹ç”»å¸ƒåæ ‡
                        pred_orig = (pred_model_truncated * [w_scale, h_scale]).astype(int)

                        if obs_orig is not None and pred_orig.shape[0] > 0:
                            start_point = obs_orig[-1]
                            full_pred = np.vstack([start_point, pred_orig])
                            for k in range(len(full_pred) - 1):
                                cv2.line(canvas, tuple(full_pred[k]), tuple(full_pred[k + 1]), (0, 0, 255), 2)

                cv2.imshow(window_name, canvas)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                shutdown_event.set()
                break
    finally:
        print(f"====== è¡Œç¨‹ {trip_id} å·²çµæŸ ======")
        shutdown_event.set()
        for thread in receiver_threads:
            thread.join(timeout=2)

        # ã€ã€ã€ æ ¸å¿ƒä¿®æ”¹ 3ï¼šä¿®æ”¹ finally å€å¡Š ã€‘ã€‘ã€‘
        # ç§»é™¤èˆŠçš„ video_writers è™•ç†é‚è¼¯
        # æ–°å¢å° event_recording_status çš„æª¢æŸ¥
        for path in active_paths:
            status = event_recording_status.get(path)
            if status and status.get("writer") is not None:
                print(f"[{path}] ç¨‹å¼çµæŸï¼Œå¼·åˆ¶åœæ­¢ä¸¦å„²å­˜æ­£åœ¨é€²è¡Œçš„äº‹ä»¶éŒ„å½±...")
                writer_to_stop = status["writer"]
                writer_to_stop.stop()
                writer_to_stop.join() # åœ¨ä¸»åŸ·è¡Œç·’ç­‰å¾…ï¼Œç¢ºä¿å½±ç‰‡å¯«å®Œå†çµæŸ
                # é€™è£¡ä¹Ÿå¯ä»¥è§¸ç™¼æœ€å¾Œä¸€æ¬¡ä¸Šå‚³
                video_filename = os.path.basename(writer_to_stop.output_path)
                relative_path = os.path.join(VIDEO_FILES_DIR, video_filename).replace("\\","/")
                upload_data = { "trip_id": trip_id, "path": path, "relative_path": relative_path, "title": f"å±éšªäº‹ä»¶ - {path}", "date": datetime.now().strftime('%m/%d'), "description": f"ç¨‹å¼çµæŸæ™‚å„²å­˜çš„ç‰‡æ®µéŒ„å½±ã€‚"}
                notify_backend(UPLOAD_VIDEO_ENDPOINT, upload_data)

        notify_backend(RECORDING_STATUS_ENDPOINT, {"session_id": trip_id, "status": "end"})
        cv2.destroyAllWindows()
        print("ç¨‹å¼å·²çµæŸã€‚")

if __name__ == "__main__":
    main()